# Python学习笔记

之前做Maya工具的时候现学现卖用了一波Python，但是没有很熟。

如今神经网络这套东西非常热门，也确实非常强大，如果不跟上时代很容易被淘汰吧。

而且我本人对神经网络还挺感兴趣的，开学！



---



# Python基本通识

从Maya笔记Copy过来的，也做了一波整理和修改。



恨啊，当初没有好好学，现在看很多地方都需要Python。看上去比较好上手，慢慢不上不了解的地方吧。

 

## form import、导入其他文件内的函数

```python
# helper.py

def square(x):

  return x ** 2

 
# main.py

from helper import square

 

result = square(5)

print(result)
# Output: 25
```

import后面是可以**加逗号然后并列**添加的，就可以一次导入多条函数。

如果想导入文件中的**所有函数**，使用**通配符***，from helper import *

关于通配符，可以参考：[通配符](##通配符)。



一般from后面的叫模块名，模块名一般就是这个py文件的文件名。

但是如果你直接从这个脚本开始运行：

![image-20240129192930258](./Images/image-20240129192930258.png) 

当前的模块名就会变成`__main__`,  这里主要就是供解释器分辨这个文件是作为主程序运行还是作为模块被导入其他程序运行。

这里的用法类似下面的“魔术方法、特殊方法”。



## 关于Python的导入缓存导致的模块热更新失效问题

由于Python自身的优化机制，在文件中导入其他模块时，Python会**缓存导入的模块**。这导致如果我更改了被引用模块，这个更改将不起效果，因为文件使用的是旧的缓存的模块。

为了解决这个问题，可以使用：

```python
import imp

import Main

imp.reload(Main)
```

这样的方式来**强制重新导入文件。**

虽然可能**降低效率**，但是能**解决热更新的问题**。

在基本确定不更改后，可以删除重新导入命令。

这里imp是一个Python自带的模块，不过在Py3.12被删掉了，妈的。

那3.12之后怎么办？——我的建议是使用3.11。^^

 

## 元组和列表

之前一直没有很好的理解，只知道元组可以放任何类型。

Python中的元组和列表都是容器类型的数据结构，用于存储多个元素，但它们有着不同的特点。

**元组（tuple）：**

- 元组是**不可变**的，即一旦创建就不能修改。

- 元组使用小括号**()**来表示，元素之间用逗号分隔。

- 元组可以包含任何类型的数据，包括其他元组。

- 元组支持索引和切片操作。

**列表（list）：**

- 列表是**可变的**，可以动态添加、删除、修改元素。


   - 列表使用方括号**[]**来表示，元素之间用逗号分隔。


   - 列表可以包含任何类型的数据，包括其他列表。


   - 列表支持索引和切片操作，同时还支持排序、插入、删除、统计等操作。

**共同点：**

- 都是容器类型的数据结构，用于存储多个元素。

- 都可以包含任何类型的数据，包括其他元组或列表。

- 都支持索引和切片操作。


**不同点：**

- 元组是不可变的，而列表是可变的。

- 元组使用小括号()来表示，列表使用方括号[]来表示。

- 元组不能添加、删除、修改元素，而列表可以。

- **元组相对于列表来说更加轻量级**，所以在需要不可变序列的情况下，使用**元组会更加高效**。

之前一直以为元组是无序不可重复的，后来发现那个叫做Set、类似哈希表吧。




## 魔术方法、特殊方法

常见的被双下划线包住的方法就是魔术方法，它们很像Unity内置的各种回调，你只要写这个函数的内容，这个函数会自动在指定的条件下被调用。

如：

```python
class People(object):

	# 创建对象
	def __new__(cls, *args, **kwargs):

    	print("触发了构造方法")

    	ret = super().__new__(cls) # 调用父类的__new__()方法创建对象

    	return ret ## 将对象返

 	# 实例化对象
    # 这里注意参数，第一个是self
	def __init__(self, name, age):

   		self.name = name

   		self.age = age

   		print("初始化方法")

 	# 删除对象

 	#  del 对象名或者程序执行结束之后

	def __del__(self):

    	print("析构方法，删除对象")
	
    # 特殊变量，__name__是这个模块的名字

	if __name__ == '__main__':

  		p1 = People('xiaoming', 16)
```

 

## 保护变量和私有变量

默认的变量声明是公开的，类内类外都可以访问。

**单下划线开头的变量是保护的，类外可以访问，但不应该访问**，因为之前的开发者不希望你访问，纯看自觉。

**双下划线开头的变量是私有的，类外可以访问，但是访问时需要做一步处理**，同上，开发者不希望你访问。

```python
class test :

  _test = 1

  __main = 2

 

def main() :

  te = test()

  print (te._test)

  print (te._test__main)

# 可以成功打印

# 对于双下划线开头变量，需要在访问时在变量名前加上“_所属类名”
```

 

## 函数返回类型声明

**Python不指定函数返回类型**，想返回什么就返回什么，也可以不返回直接结束。

在Python3.5之后，开发者可以在函数声明中加入**返回类型注解**，类似注释，有没有都**不会影响代码的编译**。

编写注解后，能在调用该函数的地方看到返回类型的提示，仅此而已。

```python
def add(x: int, y: int) -> int:

  	return x + y
```

只能说怪Python的IDE太好用了吧，不用写人也标出来了==

 ![image-20240129193923347](./Images/image-20240129193923347.png)  



## 关于pass占位符

有时候一个函数就是不需要任何内容，但是它又必须被声明和定义，那里面写什么呢？

Python又不认可空行，只留下空行的话解释器会报错，这时候写下占位符pass即可：

```python
if True:

 	pass

  	print("Hello, world!")
```

需要注意，如果写了pass，后面的代码就不会再运行了。

 

## 关于参数顺序

如果你在调用函数时指定参数的名称，是可以随意打乱参数的顺序的，这叫做关键字参数。表现上很像Maya大部分cmds函数的flag形式。

```python
def my_func(a, b, c):

  print(a, b, c)

 

# 关键字参数

my_func(c=3, a=1, b=2) #输出 1 2 3

 

# 默认参数

def my_func(a, b=2, c=3):

  print(a, b, c)

 

my_func(1) # 输出 1 2 3

my_func(1, c=4) # 输出 1 2 4
```

 详细请看下一条目，无名参数和有名参数.



## 关于无名参数和有名参数

用cmds的API的时候，发现这个函数声明看不懂，于是问了GPT，才知道Python还有有名参数和无名参数这么一说。

![截图.png](./Images/clip_image002.gif) 

*args是无名参数列表、**kwargs是有名参数字典（关键字参数）。

```python
def fun (*args, **kwargs) :

  	pass

 
fun(1,2,3,a = 1, b = 2, c = 3)
```

如上函数声明和调用，前三个是无名参数，后三个是有名参数（关键字参数）。

*和**是解包运算符，用于把列表和字典解包成参数列表，这样才能适配上各种情况下的调用。

在上面这种情况下，**args是一个列表，kwargs是一个字典，可以用于遍历。**

在定义中使用**kwargs之后，还是可以继续添加参数的：

```python
def fun (*args, **kwargs, name = "Init") :
  	pass

fun(1,2,3,a = 1, b = 2, c = 3, name = "111")
```

**根据规范，调用函数时一般无名参数在前，关键字参数在后，交替出现可能会出现匹配不上的问题。**

 

## 关于装饰器

这是Python特有的功能，当一个函数上面有@……的标志，代表这个函数被装饰了，真实运行的函数不是这个函数，而是被装饰后的函数。如下范例：

```python
# 装饰器定义函数
def my_decorator(func):
    # 定义包裹函数
  	def wrapper():
        # 使用两条Print语句包裹原函数
    	print("Before the function is called.")
    	func()
    	print("After the function is called.")
	#将重新定义后的函数返回
  	return wrapper

 

# 呼叫装饰器，此函数不再按照原函数运行
@my_decorator
def say_hello():
  print("Hello!")

 

say_hello()
# 输出：
# Before the function is called.
# Hello!
# After the function is called.
```

这么做会**降低代码的可读性**，毕竟你把其他部分弄出去了，不是所见即所得。但是在一些特殊的情况下，如：不太方便修改源码、只能通过加码的方式修改功能时，还算能用。

 

## 关于拉姆达表达式

C#中拉姆达表达式是这样：

`var fun = (a,b) => {print(a,b);};`

fun是一个委托类型，它保存一个函数。后面的a => {print(a);}是一个拉姆达表达式，a和b是参数，后面是函数体。

Python则这样写：

`fun = lambda a,b : print(a,b)`

Python中拉姆达表达式只能有1行，返回值的类型就是这个表达式的值。

 

## 关于长名和短名

根据Python的优良传统，调用函数时，若使用关键字参数，常使用关键字的短名（1~3个字母）（用长名也不会出问题）。

我很好奇这是怎么实现的，于是问了一下ChatGPT。

**直接链接长短名：**

```python
def Say(**kwarg) : 

  	\# 先试着get这个s，如果没有就试着get这个str

  	str = kwarg.get('s', kwarg.get('str', None))

  	print (str)

 
Say(str = "str")

Say(s = "s")

# Output:

# str

# s
```

这个方法相对较快，但是可能会出现同时使用长短名的问题，限定方面会出现一点困难。

 

**使用装饰器实现：**

```python
def SayShortName(Say):

  	def wrapper(**kwarg):

    	if 's' in kwarg:

      		kwarg['str'] = kwarg.pop('s', None)

   			Say(**kwarg)

  	return wrapper

 

@SayShortName

def Say(str) : 

  print (str)



Say(str = "str")

Say(s = "s")

# Output:

# str

# s
```

其实很好理解，就是使用装饰器修改一下原函数，并做一步判断：如果关键字参数字典中使用的是长名，就不动它，如果关键字字典中使用的是短名，就把短名拿到的值赋给长名的键，然后删除短名键值对避免多出一个参数（pop函数的功能就是删除并返回值），然后用处理过的参数去运行被装饰的函数就行了。

这个方法还是相对复杂，而且会降低一些代码的可读性，毕竟需要多写一个函数并做标记。

 

**Maya中的实现方式**

我又去Maya里测试了一番，发现即使同时使用长短名也不会报错，此时已先出现的为准，先用了短名就由短名决定这个flag的值。

结合Maya函数的声明，可以猜测Maya使用的是上述的第一种方式。

 

## 获取列表长度

`len(sl)`

和面向对象的sl.len的方法不同，每次我都搞错……

 

## 类型转换

类似C#

`string_num = "123"`

`int_num = int(string_num)`

 

## Logging

**仅使用print和cmds的warning和error是低效且不规范的。**

在一个工具项目中，我定义了一个变量DEBUG，用于控制开发者模式下的Log和Debug输出，这导致每次Log都需要再写一行if，非常的麻烦。

询问GPT解决方案后，发现Python有一个叫做Logging的内置库。

总结：

  <img src="./Images/截图.png" alt="截图" style="zoom:150%;" /> 

 

Handler可以玩得很花：

1. StreamHandler：将日志消息输出到控制台
2. FileHandler：将日志消息输出到文件
3. RotatingFileHandler：将日志消息输出到指定的文件，可以限制文件大小，达到一定大小后自动切割文件
4. TimedRotatingFileHandler：将日志消息输出到指定的文件，可以根据时间进行切割，比如每天或每小时切割一次
5. SocketHandler：将日志消息发送到指定的网络地址和端口
6. SMTPHandler：将日志消息发送到指定的邮箱地址

 

logging也可以用在Maya的脚本开发中，效果如下：

![截图.png](./Images/clip_image006.gif)

只不过，输出的字符串在Maya中会再做一步处理，头部会加上# 

并且，Maya会独立输出一行（在logging之后），标注输出的文件和输出内容。

**重复令人烦躁，可以通过调整Handler避免把信息输出到终端，因为终端的信息会同时被Maya和终端打印，造成重复。**

 

**要注意：**

如果Log带**中文，需要py文件首行注释文件编码方式**，如：# encoding: gbk。

在Maya使用时，由于缓存等持续运行机制，旧的Logger不会被自动删除，这导致同一个信息会被多次输出，为了解决这个问题，可能需要删除一下旧的Logger：

```python
logging.Logger.manager.loggerDict.pop(__name__, None)
logger = logging.getLogger(__name__)
```

 

## 通配符

阉割版正则表达式，可以做**简单的字符串匹配**。

在常用的场合，作为一个条件被使用。

比如：

 通过名字筛选出所有的Source

`source = cmds.ls("S_*", type = "transform")[0]`

表示用`“S_*”`为条件去筛选场景中的所有物体，`“S_*”`表示物体名必须以“S_”开头，后面是什么都无所谓。

 

GPT说明的常用场景：

在Python中，通配符常用于文件路径的匹配，例如使用

glob模块的通配符来查找符合某个模式的文件名或路径名。例如，

***.txt表示匹配所有以.txt结尾的文件名；**

**?表示匹配一个字符；**

**[abc]表示匹配字符a、b或c中的任意一个。**

通配符也可以用于数据处理中的筛选和过滤。

 

## Try

与其他高级编程语言类似的，Python也有异常抛出机制：

```python
# 通过名字筛选出所有的Target
try:
  # 可能会引发异常的代码
  x = 1 / 0
except ZeroDivisionError:
  # 处理 ZeroDivisionError 异常
  print("除数不能为零")
finally:
  # 无论异常是否被引发，都需要执行的代码
  print("程序结束")
```

如果想要捕获特定类型的错误：

- SyntaxError：代码语法错误。

- NameError：尝试访问不存在的变量或函数。

- TypeError：操作或函数应用于不适当的数据类型。

- ValueError：操作或函数应用于正确类型的数据，但该数据具有不合适的值。

- ZeroDivisionError：试图在除数为零的情况下执行整数或浮点数除法。

- IndexError：尝试访问列表、元组或字符串中不存在的索引。

- KeyError：尝试访问字典中不存在的键。

- AttributeError：尝试访问对象不存在的属性或方法。

- ImportError：无法导入指定的模块或包。

- FileNotFoundError：尝试打开不存在的文件。




如果**不知道错误什么类型**，反正都想捕获，那么**except后直接写冒号**即可。

 

## 在一行中使用if语句

`item = source if args[0] == "source" else target`

类似C#中 A ： B ？ Bool的用法

 

## 注释 

https://www.runoob.com/python3/python3-comment.html

```python
#单行注释

'''
多行注释1
'''


"""
多行注释2
"""
```



## DocString

类似C#的summary吧，但是它写在声明的下面。

一段看懂：

```python
# 生成回归用数据
def generate_data(ws, b=0, n=128):
    """
    :param ws: [1,2] 给定包含所有权重w的列表
    :param b: 0.5 偏置项
    :param n: 数据组数量
    :return: 生成的数据组，最后一列是标注。
    """
    x = np.random.rand(n, len(ws))
    y = (x * ws).sum(axis=1) + b
    datas = np.column_stack((x, y))
    datas = datas.astype(np.float32)
    return datas
```

用于类、方法的声明。

![image-20240129195335857](./Images/image-20240129195335857.png) 



## 列表切片 

```python
>>> print(a)
[1, 2, 3, 4, 99]
>>> a[0:2] # 获取索引为0到2（不包括2！）的元素
[1, 2]
>>> a[1:] # 获取从索引为1的元素到最后一个元素
[2, 3, 4, 99]
>>> a[:3] # 获取从第一个元素到索引为3（不包括3！）的元素
[1, 2, 3]
>>> a[:-1] # 获取从第一个元素到最后一个元素的前一个元素之间的元素
[1, 2, 3, 4]
>>> a[:-2] # 获取从第一个元素到最后一个元素的前二个元素之间的元素
[1, 2, 3]
```



## 字典 

```python
>>> me = {'height':180} # 生成字典
>>> me['height'] # 访问元素
180
>>> me['weight'] = 70 # 添加新元素
>>> print(me)
{'height': 180, 'weight': 70}
```



## 逻辑运算或且非

跟C#不太一样：

```python
>>> hungry = True # 饿了？
>>> sleepy = False # 困了？
>>> type(hungry)
<class 'bool'>
>>> not hungry
False
>>> hungry and sleepy # 饿并且困
False
>>> hungry or sleepy # 饿或者困
True

```



## 反斜杠换行

有时候一行代码实在太长了，可以用反斜杠换行或者括号换行：

```python
 (x_train, t_train), (x_test, t_test) = \
 load_mnist(normalize=True, flatten=True, one_hot_label=False)

plt.hist(
    total_vertex_count,
    bins=
    range(
        segre_max_total_vertex,
        max(total_vertex_count) + big_step_total_vertex,
        big_step_total_vertex
    )
)
    
```





# 常用的Package



**Numpy**

经常被`import numpy as np`

主要提供数学计算和数据分析相关的方法。



---



# 感知机

本质就是这个，输入的每个维度乘以设定好的权重，大于阈值输出1，小于等于阈值输出0

![image-20240130205006297](./Images/image-20240130205006297.png) 

不过一个这样的函数叫做单层感知机，有些东西单层感知机做不出来，比如说异或门的判断。

异或门即输入的不同则返回1，输入的相同则返回0。

这个可以用函数图像看出来：

比如下面经典的示例，三角代表返回1，圆代表返回0. 然后其实一个感知机在这里可以简单地理解为一个线性的函数：

$f(x_1,x_2) = x_1*w_1 + x_2*w2$

如果输入的x1 和x2加权累加后小于阈值了，就是下图的灰色区域，就返回0，而如果大于阈值了，就返回1.

下图就是或门被一维感知机顺利区分的示意图：

![image-20240131193219498](./Images/image-20240131193219498.png) 

可以看到一维的感知机，其fx其实就是简单的线性函数，就是一条直线，它可以切开与门、或门、与非门：

<img src="./Images/image-20240131194358159.png" alt="image-20240131194358159" style="zoom:50%;" /> <img src="./Images/image-20240131194520369.png" alt="image-20240131194520369" style="zoom:50%;" />



但是异或门是这样的：

![image-20240131194601103](./Images/image-20240131194601103.png) 

一条普通的线性函数看来是没法分开的，所以这里引入概念多层感知机。

![image-20240131194917737](./Images/image-20240131194917737.png) 



---



# 激活函数

神经网络中有一个重要的概念，叫做激活函数。

它的实质作用大概就是如图的h()：

![image-20240131201700075](./Images/image-20240131201700075.png) 

一句话就是对输入做完加权累加再加偏置后，对结果做一个处理的函数。

激活函数必须是非线性函数。



**为什么要用激活函数？为什么一定得是非线性的函数才行？**

因为单层的神经网络很鸡肋，类似单层的感知机那样，连亦或门都模拟不出来，

而如果想要使用多层感知机，就必须引入一个非线性的激活函数。

在感知机中，所谓的“大于阈值返回1，否则返回0”就是典型的非线性激活函数Step。

如果没有这个Step，我们就无法通过两层感知机模拟异或门。

如果没有这个Step，加权累加再加偏置这个函数完全就是线性的，那不管有几层，实质上和一层没什么区别，这是线性函数的特性，在学Shader入门精要的时候也有提及。

$f(x_1,x_2) = x1*w_1 + x_2*w2$ 典型的线性函数

因此，引入激活函数后，顺利地为原先是线性的这个加权累加再加偏置的流程引入了非线性特性，使得多层神经网络成为可能。

目前存在的比较知名的激活函数就是Step、Sigmod和ReLU，算起来都很容易，目前主流使用的是ReLU，以前主流的是Sigmod。



你想过为什么没有人用Step当激活函数吗

这是因为Step并非是一个连续的函数，其存在一个突变。

然而在神经网络反向传播的时候，很多时候对权重和偏置的调整只是微小值，微小的变化很难让Step产生突变，只有临界值才会产生结果的变化。

因此在反向传播的过程中，很多层的偏导会被认为是0，即对结果毫无影响。



---



# 回归问题和分类问题

神经网络拟解决的问题一般就两类，回归问题和分类问题。

简单来说就是，回归问题是预测类问题，根据输入信息，内部哐哐一顿算后给出一个结果。

比如以下几个问题就是回归问题：

1. 预测房屋价格：基于房屋的特征（如面积、地理位置、房间数量等）预测房屋的销售价格。
2. 销售预测：基于广告支出、季节性因素等，预测产品的销售额。
3. 股票价格预测：利用历史股票价格、交易量等数据，预测未来股票价格的走势。
4. 医疗预测：根据患者的生理特征、病史等信息，预测患者患某种疾病的风险或预期寿命。
5. 人口增长预测：基于过去的人口数据和经济指标，预测未来某地区的人口增长情况。

分类问题就很好理解了，给一个输入，判断数据属于哪一个预先指定的类型。

比如输入一张图，问模型这图里是猫还是狗。



根据问题的不同，输出层的激活函数设计也不同。

回归问题一般用恒等函数做原样输出，就是什么都不做就输出。

分类问题一般用SoftMax函数：

![image-20240202200533095](./Images/image-20240202200533095.png) 

这里累加1~n就是遍历最后一层的每一个数据，做EXP后累加。

exp图像是这样的：

![image-20240202201007241](./Images/image-20240202201007241.png) 



唉，其实主要目的就是把各个结果都考虑上，然后压缩到0~1并且总和为1罢了。做exp是为了让结果在大值的地方有更明显的差异，并且负值也能很好Hold住罢了。



---



# 矩阵和神经网络数据传递

前面也说了，数据在神经网络间传递就是输入数据加权求和再加偏置，然后过一遍激活函数。

这个其实用矩阵非常好写：

![image-20240202193629908](./Images/image-20240202193629908.png) 

写成矩阵的 A = XW + B，比写一大堆小写字母好看多了。

而且这里X和W的点乘，其实展开来看就是输入数据的加权求和，巧得像矩阵点乘就是为了神经网络设计的一样。

多维数组一般有一个shape属性，指的就是从大到小来看，每个维度有多少个元素，比如：

[[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]] 的Shape 就是（2，3）。

我们知道矩阵点乘是中间相等才能乘，结果取两边的维度的，比如2X3的矩阵点乘3X2的矩阵结果是2X2的矩阵。

而神经网络的输入一般是一维数据，它的Shape一般就是（x, ）这样的，x指的是一维数组中元素的个数。

比如现在有一个3个元素的输入（3，），其实它和权重矩阵相乘的时候是：1X3 dot 3X2 = 1X2，而1X2的一维数组的Shape是（2，）。所以从经验来说，权重矩阵的shape可以改变输入的一维数组的元素个数。**Shape（3,2）的权重矩阵可以把神经元个数个数从三个转为两个，以此类推。**

其实这里单看一维数组的Shape输出一个（3，）还是怪怪的，总觉得是三行若干列，其实是一行3列；或者说成是：只有一个维度，这个维度有3个元素。



---



# 最简单的神经网络示例代码

```python
# 初始化定义神经网络
def init_network():
     network = {} # 建一个空字典
     network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # W1权重信息，和偏置一起就是所谓的模型中的一层
     network['b1'] = np.array([0.1, 0.2, 0.3])
     network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) # shape是从大到小每个维度几个元素，这个是32，W1是23
     network['b2'] = np.array([0.1, 0.2])
     network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) 
     network['b3'] = np.array([0.1, 0.2])
	 return network
    
# 前向传播
def forward(network, x):
    # 输入了 模型 和 输入数据
     W1, W2, W3 = network['W1'], network['W2'], network['W3']
     b1, b2, b3 = network['b1'], network['b2'], network['b3']
    # 第一层加权求和加偏置
     a1 = np.dot(x, W1) + b1
    # 第一层激活函数
     z1 = sigmoid(a1)
    #第二层...
     a2 = np.dot(z1, W2) + b2
     z2 = sigmoid(a2)
     a3 = np.dot(z2, W3) + b3
     #identity_function = 什么都不做. 这里是输出层激活函数,根据拟解决的问题决定
     y = identity_function(a3)
     return y
    

# 初始化字典和输入数据
network = init_network()
x = np.array([1.0, 0.5])

#做前向传播
y = forward(network, x)

print(y) # [ 0.31682708 0.69627909]

```



---



# Softmax指数溢出  对策

计算机是这样的，再高的精度对上指数函数也没辙。==

因此有如下的Trick可以解决这个问题：

```python
>>> a = np.array([1010, 1000, 990])
>>> np.exp(a) / np.sum(np.exp(a)) # softmax函数的运算
array([ nan, nan, nan]) # 没有被正确计算
>>>
>>> c = np.max(a) # 1010
>>> a - c
array([ 0, -10, -20])
>>>
>>> np.exp(a - c) / np.sum(np.exp(a - c))
array([ 9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
```

两边exp里面都减一个max（a）就行。

虽然不是很懂为什么，但管我屁事，那是数学家的事。^^



但是啊，一般Softmax用也是用在分类问题上，告诉你A的概率是多少，B的是多少。

但是往往推理的时候不想知道什么破概率，我只想知道模型给出的答案是哪一个，此时其实不需要过SoftMax，直接比最后一层的值的大小就行了。

因此这个SoftMax在大部分推理的时候是不用的，但是它在训练模型的时候还是蛮有用的，可以衡量出模型学对了多少，学歪了多少。



---



# 成批推理和矩阵

正向传播也可以说成是推理。

如果一个一个数据过推理流程，性能瓶颈可能出现在数据的转移和传播中，和Unity的渲染一样。

因此合批能大大提高推理的效率，而其可行性也非常容易验证：

以多维数组Shape为例，不合批时：

输入是一张32X32合1024长度的图

(1024, ) dot (1024, 100) = (100, )

(100, ) dot (100, 1) = (1, ) 输出用于判断一个二分类，中间有一层隐藏层，也就是权重和偏置参数。

合批时：

(100, 1024)dot(1024, 100) = (100, 100)

(100, 100) dot (100 ,1) = (100, 1)

可见合批时，输入和输出的Shape的第一个维度用于存合批的索引了，而模型（权重矩阵和偏置矩阵）不需要修改，只要修改输入就行。

比如原先输入一张2X2的图：([1,2,3,4])

现在就直接：([1,2,3,4] , [5,6,7,8]) 这样合批合成两张一起就OK。



---



# 泛化和过拟合

神经网络模型的最终目的是“泛化”。

以数字识别为例，泛化的模型可以识别更多人手写出来的数字或是打印出的数字，而泛化程度不够的模型可能只能识别出训练用数据集中的数字。只对训练用数据集起作用、泛化程度低的情况叫做“过拟合”。



---



# 损失函数定义

不管权重和偏置如何，数据进去了总会有一个结果。

评判结果和正确结果差距多大的函数就是损失函数。

常用的有均方误差和交叉熵误差。

均方误差：

![image-20240204203320247](./Images/image-20240204203320247.png) 

每维误差平方累加再除以2



交叉熵误差：

![image-20240204203718130](./Images/image-20240204203718130.png) 

![image-20240204203857543](./Images/image-20240204203857543.png) 

累加每一维度的正解乘以模型答案的对数再取负。可见，在错得比较离谱的时候——比如：一张手写5的图片，模型给的答案是5的概率是0.1%，此时交叉熵会得到非常大的值，代表这个模型性质恶劣。



从直观感受上来说，“均方误差”似乎是更加“线性”的做法，而交叉熵误差是更加“非线性”的做法。



---



# One-Hot表示

在分类问题中，训练数据往往需要标记好正确的输出（答案）。

以手写数字图像为例，现有一张手写数字“5”的图像，如果我们将其答案写为：[0,0,0,0,0,1,0,0,0,0]，即5的可能性是100%，其他数字的可能性是0，这种答案的写法叫做One-Hot表示。

相对的，模型给的答案可能是[0.1,0.1,……]这种。代表模型计算得到的、这张图是10个十进制数字中的哪一个的概率。



---



# Mini-Batch小批量学习

拥有庞大数据量却没有对应计算能力时，就需要考虑使用Mini-Batch小批量学习。

说白了就是从数据集中随机选一小批用于学习。

```python
train_size = x_train.shape[0]
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size)
x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```



---



# 梯度

对于一维函数叫做斜率或者切线，但是对于二维函数就不是这样了，因为二维函数的一点上有无数条切线

![image-20240219200404820](./Images/image-20240219200404820.png) 

比如说对于上面这个函数，下面这个就是它的梯度：

![image-20240219200502279](./Images/image-20240219200502279.png) 

梯度衡量的是二维函数在某点上的变化率。因此，**梯度向量指向的方向，就是会产生最大的变化值的方向。**

没错，二维函数的梯度也是一个二维的向量。梯度的值本身就是由函数在指定点上的偏导数组成的向量：

![image-20240219201202468](./Images/image-20240219201202468.png) 

其实，从图形学上来说，如果说二维函数的两个自变量与其函数值组成一个三维的空间（就像上面的函数图），那么梯度就是这个Mesh的法线（或者法线的反方向，取决于元素值的正负）在自变量平面上的投影。



## 梯度法

通过上面的描述我们知道，梯度向量指向该点上函数值会发生最大变化的方向。

损失函数也是函数，神经网络的目的或者说指标之一就是降低损失函数的函数值，那不是天造地设？

对于某一个输入和通过模型计算出的结果，我们用结果和正确答案计算出损失函数，然后把神经网络层中的参数和偏置都往会让函数值下降得最厉害的方向调整若干数值，然后再推理一遍，用新的结果再去算损失函数，这下损失函数的值大概率比上一次低，因为我们往梯度下降的地方调整了权重和偏置。如此反复，最终到损失函数收敛。

像这样求最小的叫梯度下降法，反过来求最大的叫梯度上升法。

更新参数的过程如下式：

![image-20240219203347815](./Images/image-20240219203347815.png) 

x0和x1是某层神经网络中的两个权重参数，f是损失函数。这里我们用f在x0方向上的偏导乘以学习率$\eta$ ,即这一次在多大程度上做调整。x1参数也做同样的操作。

这个学习率太大或太小都是不对的。

太大可以理解为“太急了”，每次的步子都迈得很大，很难走到一个精确的地方，还可能因为步子太大直接迈过了目的地，后来还需要返回来；而返回来的步子还是很大，所以可能会在这一块反复横跳，像乒乓球一样。

太小则是可以理解为“太懒了”，虽然知道前进的方向，但是奈何每次只前进一点点。要知道训练的步数（step）是有限的，这么懒的话可能最终都走不到目标点。

一般来说，开发者应该动态地调整学习率。我认为这一块非常像SSR（屏幕空间反射）中实践过的“动态光线步进”，即发现步子大了减半，发现步子小了就乘2.

怎么判断步子大没大呢？在SSR中是比较计算出的深度和片元深度那个大，计算深度大说明步子没大，片元深度大说明步子大了。

感觉梯度下降法里也差不多，上一次梯度方向和这一次梯度方向，如果夹角大说明步子大了，如果夹角小说明步子小了，总之就是想说，学习率应该是开发者自己想办法动态调整的，具体怎么调肯定有迹可循。



---



# 超参数

在梯度下降法中，了解到了 “学习率”和“学习步数”等概念。这些参数是开发者（神经网络训练家？）可以配置的、但又不是神经网络内部的权重参数和偏置参数，这些参数被称为超参数 。



---







